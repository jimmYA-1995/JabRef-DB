% Encoding: UTF-8

@InProceedings{Karras2021,
  author    = {Tero Karras and Miika Aittala and Samuli Laine and Erik H\"ark\"onen and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  booktitle = {Proc. NeurIPS},
  title     = {Alias-Free Generative Adversarial Networks},
  year      = {2021},
  comment   = {* Deal with "texture sticking" issue of current hierarchical architecture of generator.
* goal: exhibits a more natural transformation hierarchy, where the exact sub-pixel position of each feature is exclusively inherited from the underlying coarse features.

drawing on 利用，依賴
after-images 後像, 餘像, 殘留影像
page 2. 提到階層式的生成網路中不同階層的網路間，可以透過圖片邊界, StyleGAN 的 per-pixel noise input, positional encoding, aliasing 紀錄前面層的位置參考，以此位置生成高階的圖片特徵
其中，aliasing 是圖像處理領域很常見且嚴重的事情，但在 GAN 的圖片領域似乎沒有很受到重視，作者給出兩個原因
- 不理想的 upsampling filter(NN, bilinear, stride convolution) 造成些微殘留影像
- Non-linearity (e.g. ReLU, Swish) 在 point-wise 上的應用
常見的 filter 甚至是高品質的 filter 都盡可能透過放大 aliasing 達成在固定圖片座標上生成符合的紋理

作者將 Aliasing 基於 Nyquist-Shannon 下探討，因此專注於 bandlimit 的連續函數，其可以用離散取樣方式表示
* 若要解決上一段提到的問題，需要使得產生圖片細節時不依賴任何圖片座標，這相當於在 sub-pixel 平移(or 旋轉) 上有，迫使其有連續不變的性質

Equivariant 等變映射
implicit network
- 作者發現 1x1 conv 是平移旋轉不變性下，生成模型所需的關鍵工具
- Group-equivariant CNN
- 許多目前 CNN 的平移不變性研究都在 classifier

Whittaker-Shannon interpolation formula
Dirac impulse
ideal interpolation filter
continuous interpolation
sinc function
作者引用 Nyquist-Shannon 的理論，說明如何在離散和連續的表示做轉換，並進一步說將此研究限制在 [0, 1] 之間，指出由於 convolution ，會需要比 sample rate ^ 2 更大的 grid，最後提到實作上只需要比 s 大一點點的 grid 即可，不需要理論上無限空間

只要 operatio F 是 equivariant 的，我們可以將連續或離散的 representation 先經過 F. 再轉回另一個
當 F. 是 2D 空間轉換時是 equivariant 的，因此作者在研究中採用平移與旋轉，並說明旋轉離散特徵相當於旋轉 spectral ，要注意半徑不能超過 s/2 (s 為 input sample rate)

有了上述 F 的特性，我們可以從離散的 convolution 透過換算得到 連續的 convolution 可以表示成以一個離散的 kernel K 在連續的 feature 上做 convolution，並維持前面提到達成 equivariant 需要的條件

Upsampling & downsampling
radially symmetric kernel},
  file      = {:2021/Karras2021 - Alias Free Generative Adversarial Networks.pdf:PDF},
  groups    = {StyleGAN},
  keywords  = {Image synthesis, GAN},
  timestamp = {2021-10-21},
  url       = {https://nvlabs.github.io/stylegan3/},
}

@Article{Karras2019,
  author        = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  title         = {Analyzing and Improving the Image Quality of StyleGAN},
  year          = {2019},
  month         = dec,
  abstract      = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  archiveprefix = {arXiv},
  eprint        = {1912.04958},
  file          = {:Generative model/1912 StyleGAN2 - Analyzing and Improving the Image Quality of StyleGAN (Karras).pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.CV, cs.LG, cs.NE, eess.IV, stat.ML},
  primaryclass  = {cs.CV},
  readstatus    = {read},
}

@Article{Karras2020,
  author        = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},
  journal       = {NeurIPS},
  title         = {Training Generative Adversarial Networks with Limited Data},
  year          = {2020},
  month         = jun,
  abstract      = {Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.},
  archiveprefix = {arXiv},
  eprint        = {2006.06676},
  file          = {:Generative model/2012NIPS training GAN with limited data.pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.CV, cs.LG, cs.NE, stat.ML},
  primaryclass  = {cs.CV},
  readstatus    = {read},
}

@Article{Karras2018,
  author        = {Tero Karras and Samuli Laine and Timo Aila},
  journal       = {CVPR},
  title         = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  year          = {2018},
  month         = dec,
  abstract      = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  archiveprefix = {arXiv},
  eprint        = {1812.04948},
  file          = {:Generative model/1812CVPR StyleGAN - A Style-Based Generator Architecture for Generative Adversarial Networks (Karras).pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.NE, cs.LG, stat.ML},
  primaryclass  = {cs.NE},
  readstatus    = {read},
}

@Article{Karras2017,
  author        = {Tero Karras and Timo Aila and Samuli Laine and Jaakko Lehtinen},
  journal       = {ICLR},
  title         = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  year          = {2017},
  month         = oct,
  abstract      = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  archiveprefix = {arXiv},
  eprint        = {1710.10196},
  file          = {:Generative model/1710ICLR PG-GAN Progressive Growing of GANs for Improved Quality, Stability, and Variation (Karras).pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.NE, cs.LG, stat.ML},
  primaryclass  = {cs.NE},
  readstatus    = {read},
}

@Article{Abdal2020,
  author        = {Rameen Abdal and Peihao Zhu and Niloy Mitra and Peter Wonka},
  journal       = {ACM Transactions on Graphics (TOG), 2021},
  title         = {StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows},
  year          = {2020},
  month         = aug,
  abstract      = {High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes, while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this paper, in the context of conditional exploration of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating conditional exploration as an instance of conditional continuous normalizing flows in the GAN latent space conditioned by attribute features. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes on both real photographs and StyleGAN generated images. For example, for faces, we vary camera pose, illumination variation, expression, facial hair, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow to other concurrent works.},
  archiveprefix = {arXiv},
  doi           = {10.1145/3447648},
  eprint        = {2008.02401},
  file          = {:2020/Abdal2020 - StyleFlow_ Attribute Conditioned Exploration of StyleGAN Generated Images Using Conditional Continuous Normalizing Flows.pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.CV, cs.GR},
  primaryclass  = {cs.CV},
  readstatus    = {skimmed},
}

@Article{albahar2021pose,
  author     = {AlBahar, Badour and Lu, Jingwan and Yang, Jimei and Shu, Zhixin and Shechtman, Eli and Huang, Jia-Bin},
  journal    = {ACM Transactions on Graphics},
  title      = {Pose with {S}tyle: {D}etail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN},
  year       = {2021},
  file       = {:2021/albahar2021pose - Pose with Style_ Detail Preserving Pose Guided Image Synthesis with Conditional StyleGAN.pdf:PDF},
  groups     = {Pose},
  keywords   = {Image synthesis, Pose},
  priority   = {prio2},
  readstatus = {skimmed},
  timestamp  = {2021-10-20},
  url        = {https://pose-with-style.github.io/},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:StyleGAN\;0\;1\;0xff0000ff\;\;StyleGAN family\;;
1 KeywordGroup:Pose\;0\;keywords\;Pose\;0\;0\;1\;0x80b380ff\;\;Synthesis with pose condition\;;
}
