% Encoding: UTF-8

@InProceedings{Karras2021,
  author    = {Tero Karras and Miika Aittala and Samuli Laine and Erik H\"ark\"onen and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  booktitle = {Proc. NeurIPS},
  title     = {Alias-Free Generative Adversarial Networks},
  year      = {2021},
  comment   = {* Deal with "texture sticking" issue of current hierarchical architecture of generator.
* goal: exhibits a more natural transformation hierarchy, where the exact sub-pixel position of each feature is exclusively inherited from the underlying coarse features.

drawing on 利用，依賴
after-images 後像, 餘像, 殘留影像
page 2. 提到階層式的生成網路中不同階層的網路間，可以透過圖片邊界, StyleGAN 的 per-pixel noise input, positional encoding, aliasing 紀錄前面層的位置參考，以此位置生成高階的圖片特徵
其中，aliasing 是圖像處理領域很常見且嚴重的事情，但在 GAN 的圖片領域似乎沒有很受到重視，作者給出兩個原因
- 不理想的 upsampling filter(NN, bilinear, stride convolution) 造成些微殘留影像
- Non-linearity (e.g. ReLU, Swish) 在 point-wise 上的應用
常見的 filter 甚至是高品質的 filter 都盡可能透過放大 aliasing 達成在固定圖片座標上生成符合的紋理

作者將 Aliasing 基於 Nyquist-Shannon 下探討，因此專注於 bandlimit 的連續函數，其可以用離散取樣方式表示
* 若要解決上一段提到的問題，需要使得產生圖片細節時不依賴任何圖片座標，這相當於在 sub-pixel 平移(or 旋轉) 上有，迫使其有連續不變的性質

Equivariant 等變映射
implicit network
- 作者發現 1x1 conv 是平移旋轉不變性下，生成模型所需的關鍵工具
- Group-equivariant CNN
- 許多目前 CNN 的平移不變性研究都在 classifier

Whittaker-Shannon interpolation formula
Dirac impulse
ideal interpolation filter
continuous interpolation
sinc function
作者引用 Nyquist-Shannon 的理論，說明如何在離散和連續的表示做轉換，並進一步說將此研究限制在 [0, 1] 之間，指出由於 convolution ，會需要比 sample rate ^ 2 更大的 grid，最後提到實作上只需要比 s 大一點點的 grid 即可，不需要理論上無限空間

只要 operatio F 是 equivariant 的，我們可以將連續或離散的 representation 先經過 F. 再轉回另一個
當 F. 是 2D 空間轉換時是 equivariant 的，因此作者在研究中採用平移與旋轉，並說明旋轉離散特徵相當於旋轉 spectral ，要注意半徑不能超過 s/2 (s 為 input sample rate)

有了上述 F 的特性，我們可以從離散的 convolution 透過換算得到 連續的 convolution 可以表示成以一個離散的 kernel K 在連續的 feature 上做 convolution，並維持前面提到達成 equivariant 需要的條件

Upsampling & downsampling
radially symmetric kernel},
  file      = {:2021/Karras2021 - Alias Free Generative Adversarial Networks.pdf:PDF},
  groups    = {StyleGAN},
  keywords  = {Image synthesis, GAN},
  timestamp = {2021-10-21},
  url       = {https://nvlabs.github.io/stylegan3/},
}

@Article{Karras2019,
  author        = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  title         = {Analyzing and Improving the Image Quality of StyleGAN},
  year          = {2019},
  month         = dec,
  abstract      = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  archiveprefix = {arXiv},
  eprint        = {1912.04958},
  file          = {:Generative model/1912 StyleGAN2 - Analyzing and Improving the Image Quality of StyleGAN (Karras).pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.CV, cs.LG, cs.NE, eess.IV, stat.ML},
  primaryclass  = {cs.CV},
  readstatus    = {read},
}

@Article{Karras2020,
  author        = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},
  journal       = {NeurIPS},
  title         = {Training Generative Adversarial Networks with Limited Data},
  year          = {2020},
  month         = jun,
  abstract      = {Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.},
  archiveprefix = {arXiv},
  eprint        = {2006.06676},
  file          = {:Generative model/2012NIPS training GAN with limited data.pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.CV, cs.LG, cs.NE, stat.ML},
  primaryclass  = {cs.CV},
  readstatus    = {read},
}

@Article{Karras2018,
  author        = {Tero Karras and Samuli Laine and Timo Aila},
  journal       = {CVPR},
  title         = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  year          = {2018},
  month         = dec,
  abstract      = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  archiveprefix = {arXiv},
  eprint        = {1812.04948},
  file          = {:Generative model/1812CVPR StyleGAN - A Style-Based Generator Architecture for Generative Adversarial Networks (Karras).pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.NE, cs.LG, stat.ML},
  primaryclass  = {cs.NE},
  readstatus    = {read},
}

@Article{Karras2017,
  author        = {Tero Karras and Timo Aila and Samuli Laine and Jaakko Lehtinen},
  journal       = {ICLR},
  title         = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  year          = {2017},
  month         = oct,
  abstract      = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  archiveprefix = {arXiv},
  eprint        = {1710.10196},
  file          = {:Generative model/1710ICLR PG-GAN Progressive Growing of GANs for Improved Quality, Stability, and Variation (Karras).pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.NE, cs.LG, stat.ML},
  primaryclass  = {cs.NE},
  readstatus    = {read},
}

@Article{Abdal2020,
  author        = {Rameen Abdal and Peihao Zhu and Niloy Mitra and Peter Wonka},
  journal       = {ACM Transactions on Graphics (TOG), 2021},
  title         = {StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows},
  year          = {2020},
  month         = aug,
  abstract      = {High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes, while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this paper, in the context of conditional exploration of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating conditional exploration as an instance of conditional continuous normalizing flows in the GAN latent space conditioned by attribute features. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes on both real photographs and StyleGAN generated images. For example, for faces, we vary camera pose, illumination variation, expression, facial hair, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow to other concurrent works.},
  archiveprefix = {arXiv},
  doi           = {10.1145/3447648},
  eprint        = {2008.02401},
  file          = {:2020/Abdal2020 - StyleFlow_ Attribute Conditioned Exploration of StyleGAN Generated Images Using Conditional Continuous Normalizing Flows.pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.CV, cs.GR},
  primaryclass  = {cs.CV},
  readstatus    = {skimmed},
}

@Article{albahar2021pose,
  author     = {AlBahar, Badour and Lu, Jingwan and Yang, Jimei and Shu, Zhixin and Shechtman, Eli and Huang, Jia-Bin},
  journal    = {ACM Transactions on Graphics},
  title      = {Pose with {S}tyle: {D}etail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN},
  year       = {2021},
  file       = {:2021/albahar2021pose - Pose with Style_ Detail Preserving Pose Guided Image Synthesis with Conditional StyleGAN.pdf:PDF},
  groups     = {Pose},
  keywords   = {Image synthesis, Pose},
  priority   = {prio2},
  readstatus = {skimmed},
  timestamp  = {2021-10-20},
  url        = {https://pose-with-style.github.io/},
}

@Article{Esser2020,
  author        = {Patrick Esser and Robin Rombach and Björn Ommer},
  title         = {Taming Transformers for High-Resolution Image Synthesis},
  year          = {2020},
  month         = dec,
  abstract      = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .},
  archiveprefix = {arXiv},
  eprint        = {2012.09841},
  file          = {:2020/Esser2020 - Taming Transformers for High Resolution Image Synthesis.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Lin2019,
  author        = {Chieh Hubert Lin and Chia-Che Chang and Yu-Sheng Chen and Da-Cheng Juan and Wei Wei and Hwann-Tzong Chen},
  title         = {COCO-GAN: Generation by Parts via Conditional Coordinating},
  year          = {2019},
  month         = mar,
  abstract      = {Humans can only interact with part of the surrounding environment due to biological restrictions. Therefore, we learn to reason the spatial relationships across a series of observations to piece together the surrounding environment. Inspired by such behavior and the fact that machines also have computational constraints, we propose \underline{CO}nditional \underline{CO}ordinate GAN (COCO-GAN) of which the generator generates images by parts based on their spatial coordinates as the condition. On the other hand, the discriminator learns to justify realism across multiple assembled patches by global coherence, local appearance, and edge-crossing continuity. Despite the full images are never generated during training, we show that COCO-GAN can produce \textbf{state-of-the-art-quality} full images during inference. We further demonstrate a variety of novel applications enabled by teaching the network to be aware of coordinates. First, we perform extrapolation to the learned coordinate manifold and generate off-the-boundary patches. Combining with the originally generated full image, COCO-GAN can produce images that are larger than training samples, which we called "beyond-boundary generation". We then showcase panorama generation within a cylindrical coordinate system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN has a built-in divide-and-conquer paradigm that reduces memory requisition during training and inference, provides high-parallelism, and can generate parts of images on-demand.},
  archiveprefix = {arXiv},
  eprint        = {1904.00284},
  file          = {:2019/Lin2019 - COCO GAN_ Generation by Parts Via Conditional Coordinating (1).pdf:PDF},
  keywords      = {cs.LG, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Xu2020,
  author        = {Rui Xu and Xintao Wang and Kai Chen and Bolei Zhou and Chen Change Loy},
  title         = {Positional Encoding as Spatial Inductive Bias in GANs},
  year          = {2020},
  month         = dec,
  abstract      = {SinGAN shows impressive capability in learning internal patch distribution despite its limited effective receptive field. We are interested in knowing how such a translation-invariant convolutional generator could capture the global structure with just a spatially i.i.d. input. In this work, taking SinGAN and StyleGAN2 as examples, we show that such capability, to a large extent, is brought by the implicit positional encoding when using zero padding in the generators. Such positional encoding is indispensable for generating images with high fidelity. The same phenomenon is observed in other generative architectures such as DCGAN and PGGAN. We further show that zero padding leads to an unbalanced spatial bias with a vague relation between locations. To offer a better spatial inductive bias, we investigate alternative positional encodings and analyze their effects. Based on a more flexible positional encoding explicitly, we propose a new multi-scale training strategy and demonstrate its effectiveness in the state-of-the-art unconditional generator StyleGAN2. Besides, the explicit spatial inductive bias substantially improve SinGAN for more versatile image manipulation.},
  archiveprefix = {arXiv},
  eprint        = {2012.05217},
  file          = {:2020/Xu2020 - Positional Encoding As Spatial Inductive Bias in GANs.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Dhariwal2021,
  author        = {Prafulla Dhariwal and Alex Nichol},
  title         = {Diffusion Models Beat GANs on Image Synthesis},
  year          = {2021},
  month         = may,
  abstract      = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet 256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our code at https://github.com/openai/guided-diffusion},
  archiveprefix = {arXiv},
  eprint        = {2105.05233},
  file          = {:2021/Dhariwal2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:PDF},
  keywords      = {cs.LG, cs.AI, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Jacot2018,
  author        = {Arthur Jacot and Franck Gabriel and Clément Hongler},
  journal       = {In Advances in neural information processing systems (pp. 8571-8580) 2018},
  title         = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  year          = {2018},
  month         = jun,
  abstract      = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function $f_\theta$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function $f_\theta$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
  archiveprefix = {arXiv},
  eprint        = {1806.07572},
  file          = {:2018/Jacot2018 - Neural Tangent Kernel_ Convergence and Generalization in Neural Networks.pdf:PDF},
  groups        = {Theorectical},
  keywords      = {cs.LG, cs.NE, math.PR, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Domingos2020,
  author        = {Pedro Domingos},
  title         = {Every Model Learned by Gradient Descent Is Approximately a Kernel Machine},
  year          = {2020},
  month         = nov,
  abstract      = {Deep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep network weights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.},
  archiveprefix = {arXiv},
  eprint        = {2012.00152},
  file          = {:2020/Domingos2020 - Every Model Learned by Gradient Descent Is Approximately a Kernel Machine.pdf:PDF},
  groups        = {Theorectical},
  keywords      = {cs.LG, cs.NE, stat.ML, I.2.6; I.5.1},
  primaryclass  = {cs.LG},
}

@Article{Hinton2015,
  author        = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  title         = {Distilling the Knowledge in a Neural Network},
  year          = {2015},
  month         = mar,
  abstract      = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arXiv},
  eprint        = {1503.02531},
  file          = {:2015/Hinton2015 - Distilling the Knowledge in a Neural Network.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {stat.ML, cs.LG, cs.NE},
  primaryclass  = {stat.ML},
}

@Article{Mirzadeh2019,
  author        = {Seyed-Iman Mirzadeh and Mehrdad Farajtabar and Ang Li and Nir Levine and Akihiro Matsukawa and Hassan Ghasemzadeh},
  title         = {Improved Knowledge Distillation via Teacher Assistant},
  year          = {2019},
  month         = feb,
  abstract      = {Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.},
  archiveprefix = {arXiv},
  eprint        = {1902.03393},
  file          = {:2019/Mirzadeh2019 - Improved Knowledge Distillation Via Teacher Assistant.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Cho2019,
  author        = {Jang Hyun Cho and Bharath Hariharan},
  journal       = {ICCV 2019},
  title         = {On the Efficacy of Knowledge Distillation},
  year          = {2019},
  month         = oct,
  abstract      = {In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don't make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher's training early. Our results generalize across datasets and models.},
  archiveprefix = {arXiv},
  eprint        = {1910.01348},
  file          = {:2019/Cho2019 - On the Efficacy of Knowledge Distillation.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.LG, cs.CV},
  primaryclass  = {cs.LG},
}

@Article{Xie2019,
  author        = {Qizhe Xie and Minh-Thang Luong and Eduard Hovy and Quoc V. Le},
  title         = {Self-training with Noisy Student improves ImageNet classification},
  year          = {2019},
  month         = nov,
  abstract      = {We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent.},
  archiveprefix = {arXiv},
  eprint        = {1911.04252},
  file          = {:2019/Xie2019 - Self Training with Noisy Student Improves ImageNet Classification.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.LG, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{,
  author = {Cristian Bucila, Rich Caruana, Alexandru Niculescu-Mizil},
  title  = {Model Compression},
  year   = {2006},
  file   = {:2006/- Model Compression.pdf:PDF},
  groups = {Knowledge Distillation},
  url    = {https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf},
}

@Article{Yuan2019,
  author        = {Li Yuan and Francis E. H. Tay and Guilin Li and Tao Wang and Jiashi Feng},
  journal       = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2020},
  title         = {Revisiting Knowledge Distillation via Label Smoothing Regularization},
  year          = {2019},
  month         = sep,
  abstract      = {Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the student, the student can also enhance the teacher significantly by reversing the KD procedure; 2) a poorly-trained teacher with much lower accuracy than the student can still improve the latter significantly. To explain these observations, we provide a theoretical analysis of the relationships between KD and label smoothing regularization. We prove that 1) KD is a type of learned label smoothing regularization and 2) label smoothing regularization provides a virtual teacher model for KD. From these results, we argue that the success of KD is not fully due to the similarity information between categories from teachers, but also to the regularization of soft targets, which is equally or even more important. Based on these analyses, we further propose a novel Teacher-free Knowledge Distillation (Tf-KD) framework, where a student model learns from itself or manuallydesigned regularization distribution. The Tf-KD achieves comparable performance with normal KD from a superior teacher, which is well applied when a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly deployed for training deep neural networks. Without any extra computation cost, Tf-KD achieves up to 0.65\% improvement on ImageNet over well-established baseline models, which is superior to label smoothing regularization.},
  archiveprefix = {arXiv},
  eprint        = {1909.11723},
  file          = {:2019/Yuan2019 - Revisiting Knowledge Distillation Via Label Smoothing Regularization.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Rajasegaran2020,
  author        = {Jathushan Rajasegaran and Salman Khan and Munawar Hayat and Fahad Shahbaz Khan and Mubarak Shah},
  title         = {Self-supervised Knowledge Distillation for Few-shot Learning},
  year          = {2020},
  month         = jun,
  abstract      = {Real-world contains an overwhelmingly large number of object classes, learning all of which at once is infeasible. Few shot learning is a promising learning paradigm due to its ability to learn out of order distributions quickly with only a few samples. Recent works [7, 41] show that simply learning a good feature embedding can outperform more sophisticated meta-learning and metric learning algorithms for few-shot learning. In this paper, we propose a simple approach to improve the representation capacity of deep neural networks for few-shot learning tasks. We follow a two-stage learning process: First, we train a neural network to maximize the entropy of the feature embedding, thus creating an optimal output manifold using a self-supervised auxiliary loss. In the second stage, we minimize the entropy on feature embedding by bringing self-supervised twins together, while constraining the manifold with student-teacher distillation. Our experiments show that, even in the first stage, self-supervision can outperform current state-of-the-art methods, with further gains achieved by our second stage distillation process. Our codes are available at: https://github.com/brjathu/SKD.},
  archiveprefix = {arXiv},
  eprint        = {2006.09785},
  file          = {:2020/Rajasegaran2020 - Self Supervised Knowledge Distillation for Few Shot Learning.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Xu2017,
  author        = {Zheng Xu and Yen-Chang Hsu and Jiawei Huang},
  title         = {Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks},
  year          = {2017},
  month         = sep,
  abstract      = {There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information learned from a large and accurate teacher network. We propose to use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The proposed method is particularly effective for relatively small student networks. Moreover, experimental results show the effect of network size when the modern networks are used as student. We empirically study the trade-off between inference time and classification accuracy, and provide suggestions on choosing a proper student network.},
  archiveprefix = {arXiv},
  comment       = {GAN 的架構主要還是拿來訓練分類問題},
  eprint        = {1709.00513},
  file          = {:2017/Xu2017 - Training Shallow and Thin Networks for Acceleration Via Knowledge Distillation with Conditional Adversarial Networks.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.LG, cs.AI, cs.CV},
  primaryclass  = {cs.LG},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:StyleGAN\;0\;1\;0xff0000ff\;\;StyleGAN family\;;
1 KeywordGroup:Pose\;0\;keywords\;Pose\;0\;0\;1\;0x80b380ff\;\;Synthesis with pose condition\;;
1 StaticGroup:Theorectical\;0\;1\;0x000000ff\;\;\;;
1 StaticGroup:Knowledge Distillation\;0\;0\;0x00ffffff\;heat\;Knowledge(Model) distillation\;;
}
