@InProceedings{Karras2021,
  author    = {Tero Karras and Miika Aittala and Samuli Laine and Erik H\"ark\"onen and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  booktitle = {Proc. NeurIPS},
  title     = {Alias-Free Generative Adversarial Networks},
  year      = {2021},
  comment   = {* Deal with "texture sticking" issue of current hierarchical architecture of generator.
* goal: exhibits a more natural transformation hierarchy, where the exact sub-pixel position of each feature is exclusively inherited from the underlying coarse features.

drawing on 利用，依賴
after-images 後像, 餘像, 殘留影像
page 2. 提到階層式的生成網路中不同階層的網路間，可以透過圖片邊界, StyleGAN 的 per-pixel noise input, positional encoding, aliasing 紀錄前面層的位置參考，以此位置生成高階的圖片特徵
其中，aliasing 是圖像處理領域很常見且嚴重的事情，但在 GAN 的圖片領域似乎沒有很受到重視，作者給出兩個原因
- 不理想的 upsampling filter(NN, bilinear, stride convolution) 造成些微殘留影像
- Non-linearity (e.g. ReLU, Swish) 在 point-wise 上的應用
常見的 filter 甚至是高品質的 filter 都盡可能透過放大 aliasing 達成在固定圖片座標上生成符合的紋理

作者將 Aliasing 基於 Nyquist-Shannon 下探討，因此專注於 bandlimit 的連續函數，其可以用離散取樣方式表示
* 若要解決上一段提到的問題，需要使得產生圖片細節時不依賴任何圖片座標，這相當於在 sub-pixel 平移(or 旋轉) 上有，迫使其有連續不變的性質

Equivariant 等變映射
implicit network
- 作者發現 1x1 conv 是平移旋轉不變性下，生成模型所需的關鍵工具
- Group-equivariant CNN
- 許多目前 CNN 的平移不變性研究都在 classifier

Whittaker-Shannon interpolation formula
Dirac impulse
ideal interpolation filter
continuous interpolation
sinc function
作者引用 Nyquist-Shannon 的理論，說明如何在離散和連續的表示做轉換，並進一步說將此研究限制在 [0, 1] 之間，指出由於 convolution ，會需要比 sample rate ^ 2 更大的 grid，最後提到實作上只需要比 s 大一點點的 grid 即可，不需要理論上無限空間

只要 operatio F 是 equivariant 的，我們可以將連續或離散的 representation 先經過 F. 再轉回另一個
當 F. 是 2D 空間轉換時是 equivariant 的，因此作者在研究中採用平移與旋轉，並說明旋轉離散特徵相當於旋轉 spectral ，要注意半徑不能超過 s/2 (s 為 input sample rate)

有了上述 F 的特性，我們可以從離散的 convolution 透過換算得到 連續的 convolution 可以表示成以一個離散的 kernel K 在連續的 feature 上做 convolution，並維持前面提到達成 equivariant 需要的條件

Upsampling & downsampling
radially symmetric kernel},
  file      = {:2021/Karras2021 - Alias Free Generative Adversarial Networks.pdf:PDF},
  groups    = {StyleGAN},
  keywords  = {Image synthesis, GAN},
  timestamp = {2021-10-21},
  url       = {https://nvlabs.github.io/stylegan3/},
}

@Article{Karras2019,
  author        = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  title         = {Analyzing and Improving the Image Quality of StyleGAN},
  year          = {2019},
  month         = dec,
  abstract      = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  archiveprefix = {arXiv},
  eprint        = {1912.04958},
  file          = {:Generative model/1912 StyleGAN2 - Analyzing and Improving the Image Quality of StyleGAN (Karras).pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.CV, cs.LG, cs.NE, eess.IV, stat.ML},
  primaryclass  = {cs.CV},
  readstatus    = {read},
}

@Article{Karras2020,
  author        = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},
  journal       = {NeurIPS},
  title         = {Training Generative Adversarial Networks with Limited Data},
  year          = {2020},
  month         = jun,
  abstract      = {Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.},
  archiveprefix = {arXiv},
  eprint        = {2006.06676},
  file          = {:Generative model/2012NIPS training GAN with limited data.pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.CV, cs.LG, cs.NE, stat.ML},
  primaryclass  = {cs.CV},
  readstatus    = {read},
}

@Article{Karras2018,
  author        = {Tero Karras and Samuli Laine and Timo Aila},
  journal       = {CVPR},
  title         = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  year          = {2018},
  month         = dec,
  abstract      = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  archiveprefix = {arXiv},
  eprint        = {1812.04948},
  file          = {:Generative model/1812CVPR StyleGAN - A Style-Based Generator Architecture for Generative Adversarial Networks (Karras).pdf:PDF},
  groups        = {StyleGAN, dataset},
  keywords      = {cs.NE, cs.LG, stat.ML},
  primaryclass  = {cs.NE},
  readstatus    = {read},
}

@Article{Karras2017,
  author        = {Tero Karras and Timo Aila and Samuli Laine and Jaakko Lehtinen},
  journal       = {ICLR},
  title         = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  year          = {2017},
  month         = oct,
  abstract      = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  archiveprefix = {arXiv},
  eprint        = {1710.10196},
  file          = {:Generative model/1710ICLR PG-GAN Progressive Growing of GANs for Improved Quality, Stability, and Variation (Karras).pdf:PDF},
  groups        = {StyleGAN, dataset},
  keywords      = {cs.NE, cs.LG, stat.ML},
  primaryclass  = {cs.NE},
  readstatus    = {read},
}

@Article{Abdal2020,
  author        = {Rameen Abdal and Peihao Zhu and Niloy Mitra and Peter Wonka},
  journal       = {ACM Transactions on Graphics (TOG), 2021},
  title         = {StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows},
  year          = {2020},
  month         = aug,
  abstract      = {High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes, while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this paper, in the context of conditional exploration of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating conditional exploration as an instance of conditional continuous normalizing flows in the GAN latent space conditioned by attribute features. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes on both real photographs and StyleGAN generated images. For example, for faces, we vary camera pose, illumination variation, expression, facial hair, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow to other concurrent works.},
  archiveprefix = {arXiv},
  doi           = {10.1145/3447648},
  eprint        = {2008.02401},
  file          = {:2020/Abdal2020 - StyleFlow_ Attribute Conditioned Exploration of StyleGAN Generated Images Using Conditional Continuous Normalizing Flows.pdf:PDF},
  groups        = {StyleGAN},
  keywords      = {cs.CV, cs.GR},
  primaryclass  = {cs.CV},
  readstatus    = {skimmed},
}

@Article{albahar2021pose,
  author     = {AlBahar, Badour and Lu, Jingwan and Yang, Jimei and Shu, Zhixin and Shechtman, Eli and Huang, Jia-Bin},
  journal    = {ACM Transactions on Graphics},
  title      = {Pose with {S}tyle: {D}etail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN},
  year       = {2021},
  file       = {:2021/albahar2021pose - Pose with Style_ Detail Preserving Pose Guided Image Synthesis with Conditional StyleGAN.pdf:PDF},
  groups     = {Pose},
  keywords   = {Image synthesis, Pose},
  priority   = {prio2},
  readstatus = {skimmed},
  timestamp  = {2021-10-20},
  url        = {https://pose-with-style.github.io/},
}

@Article{Esser2020,
  author        = {Patrick Esser and Robin Rombach and Björn Ommer},
  title         = {Taming Transformers for High-Resolution Image Synthesis},
  year          = {2020},
  month         = dec,
  abstract      = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .},
  archiveprefix = {arXiv},
  eprint        = {2012.09841},
  file          = {:2020/Esser2020 - Taming Transformers for High Resolution Image Synthesis.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Lin2019,
  author        = {Chieh Hubert Lin and Chia-Che Chang and Yu-Sheng Chen and Da-Cheng Juan and Wei Wei and Hwann-Tzong Chen},
  title         = {COCO-GAN: Generation by Parts via Conditional Coordinating},
  year          = {2019},
  month         = mar,
  abstract      = {Humans can only interact with part of the surrounding environment due to biological restrictions. Therefore, we learn to reason the spatial relationships across a series of observations to piece together the surrounding environment. Inspired by such behavior and the fact that machines also have computational constraints, we propose \underline{CO}nditional \underline{CO}ordinate GAN (COCO-GAN) of which the generator generates images by parts based on their spatial coordinates as the condition. On the other hand, the discriminator learns to justify realism across multiple assembled patches by global coherence, local appearance, and edge-crossing continuity. Despite the full images are never generated during training, we show that COCO-GAN can produce \textbf{state-of-the-art-quality} full images during inference. We further demonstrate a variety of novel applications enabled by teaching the network to be aware of coordinates. First, we perform extrapolation to the learned coordinate manifold and generate off-the-boundary patches. Combining with the originally generated full image, COCO-GAN can produce images that are larger than training samples, which we called "beyond-boundary generation". We then showcase panorama generation within a cylindrical coordinate system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN has a built-in divide-and-conquer paradigm that reduces memory requisition during training and inference, provides high-parallelism, and can generate parts of images on-demand.},
  archiveprefix = {arXiv},
  eprint        = {1904.00284},
  file          = {:2019/Lin2019 - COCO GAN_ Generation by Parts Via Conditional Coordinating (1).pdf:PDF},
  keywords      = {cs.LG, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Xu2020,
  author        = {Rui Xu and Xintao Wang and Kai Chen and Bolei Zhou and Chen Change Loy},
  title         = {Positional Encoding as Spatial Inductive Bias in GANs},
  year          = {2020},
  month         = dec,
  abstract      = {SinGAN shows impressive capability in learning internal patch distribution despite its limited effective receptive field. We are interested in knowing how such a translation-invariant convolutional generator could capture the global structure with just a spatially i.i.d. input. In this work, taking SinGAN and StyleGAN2 as examples, we show that such capability, to a large extent, is brought by the implicit positional encoding when using zero padding in the generators. Such positional encoding is indispensable for generating images with high fidelity. The same phenomenon is observed in other generative architectures such as DCGAN and PGGAN. We further show that zero padding leads to an unbalanced spatial bias with a vague relation between locations. To offer a better spatial inductive bias, we investigate alternative positional encodings and analyze their effects. Based on a more flexible positional encoding explicitly, we propose a new multi-scale training strategy and demonstrate its effectiveness in the state-of-the-art unconditional generator StyleGAN2. Besides, the explicit spatial inductive bias substantially improve SinGAN for more versatile image manipulation.},
  archiveprefix = {arXiv},
  eprint        = {2012.05217},
  file          = {:2020/Xu2020 - Positional Encoding As Spatial Inductive Bias in GANs.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Dhariwal2021,
  author        = {Prafulla Dhariwal and Alex Nichol},
  title         = {Diffusion Models Beat GANs on Image Synthesis},
  year          = {2021},
  month         = may,
  abstract      = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet 256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our code at https://github.com/openai/guided-diffusion},
  archiveprefix = {arXiv},
  eprint        = {2105.05233},
  file          = {:2021/Dhariwal2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:PDF},
  keywords      = {cs.LG, cs.AI, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Jacot2018,
  author        = {Arthur Jacot and Franck Gabriel and Clément Hongler},
  journal       = {In Advances in neural information processing systems (pp. 8571-8580) 2018},
  title         = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  year          = {2018},
  month         = jun,
  abstract      = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function $f_\theta$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function $f_\theta$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
  archiveprefix = {arXiv},
  eprint        = {1806.07572},
  file          = {:2018/Jacot2018 - Neural Tangent Kernel_ Convergence and Generalization in Neural Networks.pdf:PDF},
  groups        = {Theorectical},
  keywords      = {cs.LG, cs.NE, math.PR, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Domingos2020,
  author        = {Pedro Domingos},
  title         = {Every Model Learned by Gradient Descent Is Approximately a Kernel Machine},
  year          = {2020},
  month         = nov,
  abstract      = {Deep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep network weights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.},
  archiveprefix = {arXiv},
  eprint        = {2012.00152},
  file          = {:2020/Domingos2020 - Every Model Learned by Gradient Descent Is Approximately a Kernel Machine.pdf:PDF},
  groups        = {Theorectical},
  keywords      = {cs.LG, cs.NE, stat.ML, I.2.6; I.5.1},
  primaryclass  = {cs.LG},
}

@Article{Hinton2015,
  author        = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  title         = {Distilling the Knowledge in a Neural Network},
  year          = {2015},
  month         = mar,
  abstract      = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arXiv},
  eprint        = {1503.02531},
  file          = {:2015/Hinton2015 - Distilling the Knowledge in a Neural Network.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {stat.ML, cs.LG, cs.NE},
  primaryclass  = {stat.ML},
}

@Article{Mirzadeh2019,
  author        = {Seyed-Iman Mirzadeh and Mehrdad Farajtabar and Ang Li and Nir Levine and Akihiro Matsukawa and Hassan Ghasemzadeh},
  title         = {Improved Knowledge Distillation via Teacher Assistant},
  year          = {2019},
  month         = feb,
  abstract      = {Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.},
  archiveprefix = {arXiv},
  eprint        = {1902.03393},
  file          = {:2019/Mirzadeh2019 - Improved Knowledge Distillation Via Teacher Assistant.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Cho2019,
  author        = {Jang Hyun Cho and Bharath Hariharan},
  journal       = {ICCV 2019},
  title         = {On the Efficacy of Knowledge Distillation},
  year          = {2019},
  month         = oct,
  abstract      = {In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don't make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher's training early. Our results generalize across datasets and models.},
  archiveprefix = {arXiv},
  eprint        = {1910.01348},
  file          = {:2019/Cho2019 - On the Efficacy of Knowledge Distillation.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.LG, cs.CV},
  primaryclass  = {cs.LG},
}

@Article{Xie2019,
  author        = {Qizhe Xie and Minh-Thang Luong and Eduard Hovy and Quoc V. Le},
  title         = {Self-training with Noisy Student improves ImageNet classification},
  year          = {2019},
  month         = nov,
  abstract      = {We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent.},
  archiveprefix = {arXiv},
  eprint        = {1911.04252},
  file          = {:2019/Xie2019 - Self Training with Noisy Student Improves ImageNet Classification.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.LG, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{,
  author = {Cristian Bucila, Rich Caruana, Alexandru Niculescu-Mizil},
  title  = {Model Compression},
  year   = {2006},
  file   = {:2006/- Model Compression.pdf:PDF},
  groups = {Knowledge Distillation},
  url    = {https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf},
}

@Article{Yuan2019,
  author        = {Li Yuan and Francis E. H. Tay and Guilin Li and Tao Wang and Jiashi Feng},
  journal       = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2020},
  title         = {Revisiting Knowledge Distillation via Label Smoothing Regularization},
  year          = {2019},
  month         = sep,
  abstract      = {Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the student, the student can also enhance the teacher significantly by reversing the KD procedure; 2) a poorly-trained teacher with much lower accuracy than the student can still improve the latter significantly. To explain these observations, we provide a theoretical analysis of the relationships between KD and label smoothing regularization. We prove that 1) KD is a type of learned label smoothing regularization and 2) label smoothing regularization provides a virtual teacher model for KD. From these results, we argue that the success of KD is not fully due to the similarity information between categories from teachers, but also to the regularization of soft targets, which is equally or even more important. Based on these analyses, we further propose a novel Teacher-free Knowledge Distillation (Tf-KD) framework, where a student model learns from itself or manuallydesigned regularization distribution. The Tf-KD achieves comparable performance with normal KD from a superior teacher, which is well applied when a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly deployed for training deep neural networks. Without any extra computation cost, Tf-KD achieves up to 0.65\% improvement on ImageNet over well-established baseline models, which is superior to label smoothing regularization.},
  archiveprefix = {arXiv},
  eprint        = {1909.11723},
  file          = {:2019/Yuan2019 - Revisiting Knowledge Distillation Via Label Smoothing Regularization.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Rajasegaran2020,
  author        = {Jathushan Rajasegaran and Salman Khan and Munawar Hayat and Fahad Shahbaz Khan and Mubarak Shah},
  title         = {Self-supervised Knowledge Distillation for Few-shot Learning},
  year          = {2020},
  month         = jun,
  abstract      = {Real-world contains an overwhelmingly large number of object classes, learning all of which at once is infeasible. Few shot learning is a promising learning paradigm due to its ability to learn out of order distributions quickly with only a few samples. Recent works [7, 41] show that simply learning a good feature embedding can outperform more sophisticated meta-learning and metric learning algorithms for few-shot learning. In this paper, we propose a simple approach to improve the representation capacity of deep neural networks for few-shot learning tasks. We follow a two-stage learning process: First, we train a neural network to maximize the entropy of the feature embedding, thus creating an optimal output manifold using a self-supervised auxiliary loss. In the second stage, we minimize the entropy on feature embedding by bringing self-supervised twins together, while constraining the manifold with student-teacher distillation. Our experiments show that, even in the first stage, self-supervision can outperform current state-of-the-art methods, with further gains achieved by our second stage distillation process. Our codes are available at: https://github.com/brjathu/SKD.},
  archiveprefix = {arXiv},
  eprint        = {2006.09785},
  file          = {:2020/Rajasegaran2020 - Self Supervised Knowledge Distillation for Few Shot Learning.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Xu2017,
  author        = {Zheng Xu and Yen-Chang Hsu and Jiawei Huang},
  title         = {Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks},
  year          = {2017},
  month         = sep,
  abstract      = {There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information learned from a large and accurate teacher network. We propose to use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The proposed method is particularly effective for relatively small student networks. Moreover, experimental results show the effect of network size when the modern networks are used as student. We empirically study the trade-off between inference time and classification accuracy, and provide suggestions on choosing a proper student network.},
  archiveprefix = {arXiv},
  comment       = {GAN 的架構主要還是拿來訓練分類問題},
  eprint        = {1709.00513},
  file          = {:2017/Xu2017 - Training Shallow and Thin Networks for Acceleration Via Knowledge Distillation with Conditional Adversarial Networks.pdf:PDF},
  groups        = {Knowledge Distillation},
  keywords      = {cs.LG, cs.AI, cs.CV},
  primaryclass  = {cs.LG},
}

@InProceedings{Richardson2021,
  author   = {Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or},
  title    = {Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation},
  year     = {2021},
  abstract = {IEEE Conference on Computer Vision and Pattern Recognition},
  file     = {:/home/yct/OneDrive/Paper/2021/Richardson_Encoding_in_Style_A_StyleGAN_Encoder_for_Image-to-Image_Translation_CVPR_2021_paper.pdf:PDF},
  groups   = {StyleGAN-is-good},
}

@Article{article,
  author  = {Hermosilla, Gabriel and Henriquez, Diego-Ignacio and Allende-Cid, Héctor and Farias, Gonzalo and Vera, Esteban},
  journal = {IEEE Access},
  title   = {Thermal Face Generation using StyleGAN},
  year    = {2021},
  month   = {06},
  pages   = {1-1},
  volume  = {PP},
  doi     = {10.1109/ACCESS.2021.3085423},
  file    = {:2021/Thermal_Face_Generation_using_StyleGAN.pdf:PDF},
  groups  = {StyleGAN-is-good},
}

@Article{Tewari2020,
  author        = {Ayush Tewari and Mohamed Elgharib and Gaurav Bharaj and Florian Bernard and Hans-Peter Seidel and Patrick Pérez and Michael Zollhöfer and Christian Theobalt},
  title         = {StyleRig: Rigging StyleGAN for 3D Control over Portrait Images},
  year          = {2020},
  month         = mar,
  abstract      = {StyleGAN generates photorealistic portrait images of faces with eyes, teeth, hair and context (neck, shoulders, background), but lacks a rig-like control over semantic face parameters that are interpretable in 3D, such as face pose, expressions, and scene illumination. Three-dimensional morphable face models (3DMMs) on the other hand offer control over the semantic parameters, but lack photorealism when rendered and only model the face interior, not other parts of a portrait image (hair, mouth interior, background). We present the first method to provide a face rig-like control over a pretrained and fixed StyleGAN via a 3DMM. A new rigging network, RigNet is trained between the 3DMM's semantic parameters and StyleGAN's input. The network is trained in a self-supervised manner, without the need for manual annotations. At test time, our method generates portrait images with the photorealism of StyleGAN and provides explicit control over the 3D semantic parameters of the face.},
  archiveprefix = {arXiv},
  eprint        = {2004.00121},
  file          = {:2020/Tewari2020 - StyleRig_ Rigging StyleGAN for 3D Control Over Portrait Images.pdf:PDF},
  groups        = {StyleGAN-is-good},
  keywords      = {cs.CV, cs.GR},
  primaryclass  = {cs.CV},
}

@Article{Garbin2020,
  author        = {Stephan J. Garbin and Marek Kowalski and Matthew Johnson and Jamie Shotton},
  title         = {High Resolution Zero-Shot Domain Adaptation of Synthetically Rendered Face Images},
  year          = {2020},
  month         = jun,
  abstract      = {Generating photorealistic images of human faces at scale remains a prohibitively difficult task using computer graphics approaches. This is because these require the simulation of light to be photorealistic, which in turn requires physically accurate modelling of geometry, materials, and light sources, for both the head and the surrounding scene. Non-photorealistic renders however are increasingly easy to produce. In contrast to computer graphics approaches, generative models learned from more readily available 2D image data have been shown to produce samples of human faces that are hard to distinguish from real data. The process of learning usually corresponds to a loss of control over the shape and appearance of the generated images. For instance, even simple disentangling tasks such as modifying the hair independently of the face, which is trivial to accomplish in a computer graphics approach, remains an open research question. In this work, we propose an algorithm that matches a non-photorealistic, synthetically generated image to a latent vector of a pretrained StyleGAN2 model which, in turn, maps the vector to a photorealistic image of a person of the same pose, expression, hair, and lighting. In contrast to most previous work, we require no synthetic training data. To the best of our knowledge, this is the first algorithm of its kind to work at a resolution of 1K and represents a significant leap forward in visual realism.},
  archiveprefix = {arXiv},
  eprint        = {2006.15031},
  file          = {:2020/Garbin2020 - High Resolution Zero Shot Domain Adaptation of Synthetically Rendered Face Images.pdf:PDF},
  groups        = {StyleGAN-is-good},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Liu2021,
  author        = {Yahui Liu and Yajing Chen and Linchao Bao and Nicu Sebe and Bruno Lepri and Marco De Nadai},
  title         = {ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image Translation},
  year          = {2021},
  month         = sep,
  abstract      = {Recently, there has been an increasing interest in image editing methods that employ pre-trained unconditional image generators (e.g., StyleGAN). However, applying these methods to translate images to multiple visual domains remains challenging. Existing works do not often preserve the domain-invariant part of the image (e.g., the identity in human face translations), they do not usually handle multiple domains, or do not allow for multi-modal translations. This work proposes an implicit style function (ISF) to straightforwardly achieve multi-modal and multi-domain image-to-image translation from pre-trained unconditional generators. The ISF manipulates the semantics of an input latent code to make the image generated from it lying in the desired visual domain. Our results in human face and animal manipulations show significantly improved results over the baselines. Our model enables cost-effective multi-modal unsupervised image-to-image translations at high resolution using pre-trained unconditional GANs. The code and data are available at: \url{https://github.com/yhlleo/stylegan-mmuit}.},
  archiveprefix = {arXiv},
  eprint        = {2109.12492},
  file          = {:2021/Liu2021 - ISF GAN_ an Implicit Style Function for High Resolution Image to Image Translation.pdf:PDF},
  groups        = {StyleGAN-is-good},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Miyato2018,
  author        = {Takeru Miyato and Masanori Koyama},
  title         = {cGANs with Projection Discriminator},
  year          = {2018},
  month         = feb,
  abstract      = {We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (ImageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.},
  archiveprefix = {arXiv},
  eprint        = {1802.05637},
  file          = {:2018/Miyato2018 - CGANs with Projection Discriminator.pdf:PDF},
  keywords      = {cs.LG, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@InProceedings{Kazemi2014,
  author          = {Vahid Kazemi and Josephine Sullivan},
  title           = {One millisecond face alignment with an ensemble of regression trees},
  year            = {2014},
  address         = {Columbus, OH, USA},
  pages           = {1867--1874},
  publisher       = {IEEE},
  abstract        = {This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.},
  date            = {23-28 June 2014},
  doi             = {10.1109/CVPR.2014.241},
  eventdate       = {23-28 June 2014},
  eventtitleaddon = {Columbus, OH, USA},
  file            = {:2014/Kazemi2014 - One Millisecond Face Alignment with an Ensemble of Regression Trees.html:URL},
  isbn            = {978-1-4799-5118-5},
  issn            = {1063-6919},
  journal         = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  keywords        = {Shape, Regression tree analysis, Face, Training, Boosting, Training data, Vectors, Face Alignment, Real-Time, Gradient Boosting, Decision Trees},
}

@Article{Yu2015,
  author        = {Fisher Yu and Ari Seff and Yinda Zhang and Shuran Song and Thomas Funkhouser and Jianxiong Xiao},
  title         = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop},
  year          = {2015},
  month         = jun,
  abstract      = {While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.},
  archiveprefix = {arXiv},
  eprint        = {1506.03365},
  file          = {:2015/Yu2015 - LSUN_ Construction of a Large Scale Image Dataset Using Deep Learning with Humans in the Loop.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Sauer2021,
  author        = {Axel Sauer and Kashyap Chitta and Jens Müller and Andreas Geiger},
  title         = {Projected GANs Converge Faster},
  year          = {2021},
  month         = nov,
  abstract      = {Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train. They need careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space. Motivated by the finding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fr\'echet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.},
  archiveprefix = {arXiv},
  eprint        = {2111.01007},
  file          = {:2021/Sauer2021 - Projected GANs Converge Faster.pdf:PDF},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Ho2020,
  author        = {Jonathan Ho and Ajay Jain and Pieter Abbeel},
  title         = {Denoising Diffusion Probabilistic Models},
  year          = {2020},
  month         = jun,
  abstract      = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arXiv},
  eprint        = {2006.11239},
  file          = {:2020/Ho2020 - Denoising Diffusion Probabilistic Models.pdf:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Misc{Liu2016,
  author = {Ziwei Liu and Ping Luo and Shi Qiu and Xiaogang Wang and Xiaoou Tang},
  title  = {DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations},
  year   = {2016},
  doi    = {10.1109/cvpr.2016.124},
}

@Article{King2009,
  author    = {Davis E. King},
  journal   = {J. Mach. Learn. Res.},
  title     = {Dlib-ml: {A} Machine Learning Toolkit},
  year      = {2009},
  pages     = {1755--1758},
  volume    = {10},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/jmlr/King09.bib},
  url       = {https://dl.acm.org/citation.cfm?id=1755843},
}

@Misc{2019,
  title = {DEEP-ML 2019 Program Committee},
  year  = {2019},
  doi   = {10.1109/deep-ml.2019.00007},
}

@Article{Choi2019,
  author        = {Yunjey Choi and Youngjung Uh and Jaejun Yoo and Jung-Woo Ha},
  title         = {StarGAN v2: Diverse Image Synthesis for Multiple Domains},
  year          = {2019},
  month         = dec,
  abstract      = {A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at https://github.com/clovaai/stargan-v2.},
  archiveprefix = {arXiv},
  eprint        = {1912.01865},
  file          = {:2019/Choi2019 - StarGAN V2_ Diverse Image Synthesis for Multiple Domains.pdf:PDF},
  groups        = {dataset},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Gou2020,
  author        = {Jianping Gou and Baosheng Yu and Stephen John Maybank and Dacheng Tao},
  title         = {Knowledge Distillation: A Survey},
  year          = {2020},
  month         = jun,
  abstract      = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
  archiveprefix = {arXiv},
  doi           = {10.1007/s11263-021-01453-z},
  eprint        = {2006.05525},
  file          = {:2020/Gou2020 - Knowledge Distillation_ a Survey.pdf:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Wang2017,
  author        = {Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Andrew Tao and Jan Kautz and Bryan Catanzaro},
  title         = {High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs},
  year          = {2017},
  month         = nov,
  abstract      = {We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.},
  archiveprefix = {arXiv},
  eprint        = {1711.11585},
  file          = {:2017/Wang2017 - High Resolution Image Synthesis and Semantic Manipulation with Conditional GANs.pdf:PDF},
  keywords      = {cs.CV, cs.GR, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Chang2020,
  author        = {Ting-Yun Chang and Chi-Jen Lu},
  title         = {TinyGAN: Distilling BigGAN for Conditional Image Generation},
  year          = {2020},
  month         = sep,
  abstract      = {Generative Adversarial Networks (GANs) have become a powerful approach for generative image modeling. However, GANs are notorious for their training instability, especially on large-scale, complex datasets. While the recent work of BigGAN has significantly improved the quality of image generation on ImageNet, it requires a huge model, making it hard to deploy on resource-constrained devices. To reduce the model size, we propose a black-box knowledge distillation framework for compressing GANs, which highlights a stable and efficient training process. Given BigGAN as the teacher network, we manage to train a much smaller student network to mimic its functionality, achieving competitive performance on Inception and FID scores with the generator having $16\times$ fewer parameters.},
  archiveprefix = {arXiv},
  eprint        = {2009.13829},
  file          = {:2020/Chang2020 - TinyGAN_ Distilling BigGAN for Conditional Image Generation.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Huang2017,
  author        = {Xun Huang and Serge Belongie},
  title         = {Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization},
  year          = {2017},
  month         = mar,
  abstract      = {Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.},
  archiveprefix = {arXiv},
  eprint        = {1703.06868},
  file          = {:2017/Huang2017 - Arbitrary Style Transfer in Real Time with Adaptive Instance Normalization.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Lin2021,
  author        = {Chieh Hubert Lin and Hsin-Ying Lee and Yen-Chi Cheng and Sergey Tulyakov and Ming-Hsuan Yang},
  title         = {InfinityGAN: Towards Infinite-Pixel Image Synthesis},
  year          = {2021},
  month         = apr,
  abstract      = {We present a novel framework, InfinityGAN, for arbitrary-sized image generation. The task is associated with several key challenges. First, scaling existing models to an arbitrarily large image size is resource-constrained, in terms of both computation and availability of large-field-of-view training data. InfinityGAN trains and infers in a seamless patch-by-patch manner with low computational resources. Second, large images should be locally and globally consistent, avoid repetitive patterns, and look realistic. To address these, InfinityGAN disentangles global appearances, local structures, and textures. With this formulation, we can generate images with spatial size and level of details not attainable before. Experimental evaluation validates that InfinityGAN generates images with superior realism compared to baselines and features parallelizable inference. Finally, we show several applications unlocked by our approach, such as spatial style fusion, multi-modal outpainting, and image inbetweening. All applications can be operated with arbitrary input and output sizes.},
  archiveprefix = {arXiv},
  eprint        = {2104.03963},
  file          = {:2021/Lin2021 - InfinityGAN_ Towards Infinite Pixel Image Synthesis.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Liu2021a,
  author        = {Bingchen Liu and Yizhe Zhu and Kunpeng Song and Ahmed Elgammal},
  title         = {Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis},
  year          = {2021},
  month         = jan,
  abstract      = {Training Generative Adversarial Networks (GAN) on high-fidelity images usually requires large-scale GPU-clusters and a vast number of training images. In this paper, we study the few-shot image synthesis task for GAN with minimum computing cost. We propose a light-weight GAN structure that gains superior quality on 1024*1024 resolution. Notably, the model converges from scratch with just a few hours of training on a single RTX-2080 GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute our work, a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. With thirteen datasets covering a wide variety of image domains (The datasets and code are available at: https://github.com/odegeasslbc/FastGAN-pytorch), we show our model's superior performance compared to the state-of-the-art StyleGAN2, when data and computing budget are limited.},
  archiveprefix = {arXiv},
  eprint        = {2101.04775},
  file          = {:2021/Liu2021a - Towards Faster and Stabilized GAN Training for High Fidelity Few Shot Image Synthesis.pdf:PDF},
  keywords      = {cs.CV, cs.AI},
  primaryclass  = {cs.CV},
}

@Article{Yu2021,
  author        = {Jiahui Yu and Xin Li and Jing Yu Koh and Han Zhang and Ruoming Pang and James Qin and Alexander Ku and Yuanzhong Xu and Jason Baldridge and Yonghui Wu},
  title         = {Vector-quantized Image Modeling with Improved VQGAN},
  year          = {2021},
  month         = oct,
  abstract      = {Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling (VIM) approach that involves pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple improvements over vanilla VQGAN from architecture to codebook learning, yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN further improves vector-quantized image modeling tasks, including unconditional, class-conditioned image generation and unsupervised representation learning. When trained on ImageNet at 256x256 resolution, we achieve Inception Score (IS) of 175.1 and Fr'echet Inception Distance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised pretraining, we further evaluate the pretrained Transformer by averaging intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 72.2% for a similar model size. ViM-L also outperforms iGPT-XL which is trained with extra web image data and larger model size.},
  archiveprefix = {arXiv},
  eprint        = {2110.04627},
  file          = {:2021/Yu2021 - Vector Quantized Image Modeling with Improved VQGAN.pdf:PDF},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Kynkaeaenniemi2022,
  author        = {Tuomas Kynkäänniemi and Tero Karras and Miika Aittala and Timo Aila and Jaakko Lehtinen},
  title         = {The Role of ImageNet Classes in Fréchet Inception Distance},
  year          = {2022},
  month         = mar,
  abstract      = {Fr\'echet Inception Distance (FID) is a metric for quantifying the distance between two distributions of images. Given its status as a standard yardstick for ranking models in data-driven generative modeling research, it seems important that the distance is computed from general, "vision-related" features. But is it? We observe that FID is essentially a distance between sets of ImageNet class probabilities. We trace the reason to the fact that the standard feature space, the penultimate "pre-logit" layer of a particular Inception-V3 classifier network, is only one affine transform away from the logits, i.e., ImageNet classes, and thus, the features are necessarily highly specialized to them. This has unintuitive consequences for the metric's sensitivity. For example, when evaluating a model for human faces, we observe that, on average, FID is actually very insensitive to the facial region, and that the probabilities of classes like "bow tie" or "seat belt" play a much larger role. Further, we show that FID can be significantly reduced -- without actually improving the quality of results -- by an attack that first generates a slightly larger set of candidates, and then chooses a subset that happens to match the histogram of such "fringe features" in the real data. We then demonstrate that this observation has practical relevance in case of ImageNet pre-training of GANs, where a part of the observed FID improvement turns out not to be real. Our results suggest caution against over-interpreting FID improvements, and underline the need for distribution metrics that are more perceptually uniform.},
  archiveprefix = {arXiv},
  eprint        = {2203.06026},
  file          = {:2022/Kynkaeaenniemi2022 - The Role of ImageNet Classes in Fréchet Inception Distance.pdf:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG, cs.NE, stat.ML},
  primaryclass  = {cs.CV},
}

@Article{,
  author  = {Stanislav Morozov, Andrey Voynov, Artem Babenko},
  journal = {ICLR},
  title   = {On Self-Supervised Image Representations For GAN Evaluation},
  year    = {2021},
  file    = {:2021/Morozov2021 - on self supervised image repre.pdf:PDF},
  url     = {https://openreview.net/pdf?id=NeRdBeTionN},
}

@Article{Selvaraju2016,
  author        = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
  title         = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization},
  year          = {2016},
  month         = oct,
  abstract      = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  archiveprefix = {arXiv},
  doi           = {10.1007/s11263-019-01228-7},
  eprint        = {1610.02391},
  file          = {:2016/Selvaraju2016 - Grad CAM_ Visual Explanations from Deep Networks Via Gradient Based Localization.pdf:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Huang2020,
  author        = {Lei Huang and Jie Qin and Yi Zhou and Fan Zhu and Li Liu and Ling Shao},
  title         = {Normalization Techniques in Training DNNs: Methodology, Analysis and Application},
  year          = {2020},
  month         = sep,
  abstract      = {Normalization techniques are essential for accelerating the training and improving the generalization of deep neural networks (DNNs), and have successfully been used in various applications. This paper reviews and comments on the past, present and future of normalization methods in the context of DNN training. We provide a unified picture of the main motivation behind different approaches from the perspective of optimization, and present a taxonomy for understanding the similarities and differences between them. Specifically, we decompose the pipeline of the most representative normalizing activation methods into three components: the normalization area partitioning, normalization operation and normalization representation recovery. In doing so, we provide insight for designing new normalization technique. Finally, we discuss the current progress in understanding normalization methods, and provide a comprehensive review of the applications of normalization for particular tasks, in which it can effectively solve the key issues.},
  archiveprefix = {arXiv},
  eprint        = {2009.12836},
  file          = {:2020/Huang2020 - Normalization Techniques in Training DNNs_ Methodology, Analysis and Application.pdf:PDF},
  keywords      = {cs.LG, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:StyleGAN\;0\;1\;0xff0000ff\;\;StyleGAN family\;;
1 KeywordGroup:Pose\;0\;keywords\;Pose\;0\;0\;1\;0x80b380ff\;\;Synthesis with pose condition\;;
1 StaticGroup:Theorectical\;0\;1\;0x000000ff\;\;\;;
1 StaticGroup:Knowledge Distillation\;0\;1\;0x00ffffff\;heat\;Knowledge(Model) distillation\;;
1 StaticGroup:StyleGAN-is-good\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:dataset\;0\;1\;0x0000ffff\;\;\;;
}
